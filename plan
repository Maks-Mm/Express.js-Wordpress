Nice — I’ll give you a concrete, step-by-step implementation plan with exactly what to do, which files to create, commands to run, time/effort estimates, and deliverables for each step. I’ll assume you’ll implement this yourself (you said you code at a mid level), and I’ll include optional parts when you want to outsource. No questions — I’ll pick reasonable defaults (Node 18+, TypeScript optional, MongoDB Atlas, Vercel/Render). Let’s go.

Implementation plan — step-by-step (detailed)

> Goal: Extend headless WordPress + Express + React to automatically fetch Dortmund local news and render them together with WordPress posts.



Total recommended scope if you do it yourself: ~80–110 hours over 4–6 weeks part-time. I’ll break this into 8 concrete milestones with files, commands, and estimates.


---

Milestone 0 — Prep & repo (1–2 hours)

Outcome: Single monorepo or two repos (backend + frontend) with basic README and dev scripts.

1. Create repo(s)



# backend
mkdir dortmund-news-backend && cd dortmund-news-backend
git init
npm init -y

# frontend (separate or monorepo)
npx create-next-app@latest dortmund-news-frontend --typescript

2. Add basic README, license, and .env.example.


3. Setup branches: main, develop.



Estimated time: 1–2 hours


---

Milestone 1 — MongoDB + Mongoose model (3–5 hours)

Outcome: Working MongoDB connection and News model.

1. Create MongoDB Atlas cluster (free tier) and get connection string.


2. Install packages:



cd dortmund-news-backend
npm i express mongoose dotenv
npm i -D nodemon typescript @types/node @types/express

3. Add server.ts (or app.js if JS):



server.ts

import express from 'express';
import mongoose from 'mongoose';
import dotenv from 'dotenv';
dotenv.config();

const app = express();
app.use(express.json());

const MONGO = process.env.MONGODB_URI!;
mongoose.connect(MONGO)
  .then(()=> console.log('MongoDB connected'))
  .catch(err=>console.error(err));

app.get('/', (_req, res) => res.json({ ok: true }));

app.listen(process.env.PORT || 5000, ()=> console.log('Server started'));

4. Create models/News.ts:



import { Schema, model } from 'mongoose';
const newsSchema = new Schema({
  title: { type: String, required: true },
  link: { type: String, required: true, unique: true },
  description: String,
  date: { type: Date, required: true },
  source: String,
  imageUrl: String,
  scrapedAt: { type: Date, default: Date.now }
}, { timestamps: true });

export default model('News', newsSchema);

5. Add .env.example with MONGODB_URI.



Deliverable: GET / returns {ok:true}, Mongoose connected, News model ready.

Estimated time: 3–5 hours


---

Milestone 2 — Scraper service (8–14 hours)

Outcome: Service that scrapes Dortmund official pages, returns normalized items.

1. Install scraping libs:



npm i axios cheerio date-fns

2. Add services/newsScraper.ts (class-based, multiple sources):



import axios from 'axios';
import cheerio from 'cheerio';
import { parse } from 'date-fns'; // optional

export default class DortmundNewsScraper {
  sources = [
    { name: 'Stadt Dortmund', url: 'https://www.dortmund.de/de/leben_in_dortmund/medien/aktuelle_nachrichten/index.html', base: 'https://www.dortmund.de' },
    // add others
  ];

  async scrapeAll() {
    const items = [];
    for (const s of this.sources) {
      try {
        const html = (await axios.get(s.url)).data;
        const $ = cheerio.load(html);
        $('.news-list-item, .teaser, article').each((_i, el) => {
          const title = $(el).find('h2, h3, .title').text().trim();
          let link = $(el).find('a').attr('href') || '';
          if (link && !link.startsWith('http')) link = s.base + link;
          const desc = $(el).find('p').first().text().trim();
          const dateText = $(el).find('.date, time').text().trim();
          const date = dateText ? new Date(dateText) : new Date();
          if (title && link) items.push({ title, link, description: desc, date, source: s.name });
        });
      } catch (e) { console.error('scrape error', s.name, e.message); }
    }
    return this.dedupe(items);
  }

  dedupe(items) {
    const map = new Map();
    items.forEach(i => { map.set(i.link || i.title, i); });
    return Array.from(map.values());
  }
}

3. Create test endpoint /api/scrape/test that returns scraped items (without saving).


4. Manually run and inspect results — adjust selectors for Dortmund pages. (This is the fiddly part; expect multiple iterations.)



Deliverable: GET /api/scrape/test returns array of normalized news items.

Estimated time: 8–14 hours (selector tuning and testing is the main time sink)


---

Milestone 3 — Save to DB + dedupe & API endpoints (6–10 hours)

Outcome: Save scraped items to MongoDB, provide /api/news and /api/news/scrape (manual trigger).

1. Create routes/news.ts:



import express from 'express';
import DortmundNewsScraper from '../services/newsScraper';
import News from '../models/News';

const router = express.Router();
const scraper = new DortmundNewsScraper();

router.get('/', async (_req, res) => {
  const items = await News.find().sort({ date: -1 }).limit(100);
  res.json(items);
});

router.post('/scrape', async (_req, res) => {
  const scraped = await scraper.scrapeAll();
  let inserted = 0;
  for (const item of scraped) {
    try {
      await News.findOneAndUpdate({ link: item.link }, item, { upsert: true });
      inserted++;
    } catch (e) { /* ignore dupes */ }
  }
  res.json({ message:'ok', count: inserted });
});

export default router;

2. Wire route in server.ts.


3. Add basic auth / secret header for POST /scrape (optional but recommended):



if (req.headers['x-scrape-key'] !== process.env.SCRAPE_KEY) return res.status(403).end();

4. Manual test: curl -X POST http://localhost:5000/api/news/scrape -H 'x-scrape-key: yourkey'



Deliverable: Persistent News collection + API to fetch and trigger scrape.

Estimated time: 6–10 hours


---

Milestone 4 — Scheduler (node-cron) & safety (2–4 hours)

Outcome: Automatic scraping on schedule + startup run, with environment-controlled interval.

1. Install:



npm i node-cron

2. Create services/scheduler.ts:



import cron from 'node-cron';
import DortmundNewsScraper from './newsScraper';
import News from '../models/News';

export default class Scheduler {
  scraper = new DortmundNewsScraper();
  start() {
    const cronExpr = process.env.SCRAPE_CRON || '0 */6 * * *';
    cron.schedule(cronExpr, async () => {
      console.log('Running scheduled scrape...');
      const items = await this.scraper.scrapeAll();
      for (const it of items) await News.findOneAndUpdate({ link: it.link }, it, { upsert: true });
    });
    // initial call
    setTimeout(() => this.scrapeNow(), 5000);
  }
  async scrapeNow() { /* same logic */ }
}

3. Start scheduler in server.ts after DB connect.


4. Add .env keys: SCRAPE_CRON, SCRAPE_KEY.



Safety notes: add rate-limiting and user-agent header to axios to be polite:

axios.get(url, { headers: { 'User-Agent': 'MyProjectBot/1.0 (+email)' }});

Estimated time: 2–4 hours


---

Milestone 5 — Frontend integration (React / Next.js) (10–16 hours)

Outcome: Component that fetches WordPress posts + /api/news and renders combined list with tabs.

1. In your frontend repo, create component components/PostsWithNews.tsx (you already have a version — integrate it).


2. API endpoints to call:

WordPress posts: https://your-wp-site/wp-json/wp/v2/posts?per_page=20

Your backend news: https://api.yourdomain.com/api/news



3. Implement useEffect to fetch both and combine:



const [posts, setPosts] = useState([]);
const [news, setNews] = useState([]);
useEffect(()=> {
  Promise.all([
    fetch(WP_URL).then(r=>r.json()),
    fetch(NEWS_API).then(r=>r.json())
  ]).then(([wp, n])=> {
    const combined = [...wp.map(p=>({...p, type:'wp'})), ...n.map(x=>({...x, type:'news'}))]
      .sort((a,b)=> new Date(b.date).getTime() - new Date(a.date).getTime());
    setContent(combined);
  });
}, []);

4. Add tabs (All / Posts / News), AOS init, and card UI. Use provided code from earlier message.


5. Add a small badge for source and external link for news items.



Deliverable: Single feed page showing combined content with filters and item detail view.

Estimated time: 10–16 hours


---

Milestone 6 — Testing, error handling, and polish (6–10 hours)

Outcome: Robust error handling, unit / manual tests, and UX polish.

1. Backend:

Validate stored items (title, link, date) before saving.

Try/catch around DB operations.

Add simple logging (console or a logger like pino).



2. Frontend:

Loading and error states.

Link sanitation (open external links in new tab).



3. Manual QA:

Run POST /api/news/scrape and check news appear.

Check date order, duplicates, and descriptions.



4. Optional: Add tests (Jest) for critical functions (scraper parsing).



Estimated time: 6–10 hours


---

Milestone 7 — Deployment & CI (6–10 hours)

Outcome: Production deployment, secured scrape trigger, and CI pipeline.

1. Backend deployment options:

Render/Railway (easy): connect repo, set env vars.

VPS (Hetzner/Contabo): Dockerize.



2. Example Dockerfile:



FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
CMD ["node", "dist/server.js"]

3. GitHub Actions for CI (build & test) and optional scheduled workflow to trigger scrape:



# .github/workflows/cron-scrape.yml
on:
  schedule:
    - cron: '0 */6 * * *'
jobs:
  trigger:
    runs-on: ubuntu-latest
    steps:
      - name: Trigger scrape
        run: curl -X POST -H "x-scrape-key:${{ secrets.SCRAPE_KEY }}" https://api.yourdomain.com/api/news/scrape

4. Configure environment variables in host: MONGODB_URI, SCRAPE_KEY, SCRAPE_CRON, NODE_ENV=production.



Deliverable: Live backend on a public URL with scheduled scraping and frontend live.

Estimated time: 6–10 hours


---

Milestone 8 — Monitoring & Maintenance (ongoing)

Outcome: Alerts and plan for long term.

1. Monitoring: UptimeRobot for the backend, logs via Papertrail/LogDNA (optional).


2. Backups: MongoDB Atlas automated backups.


3. Maintenance tasks:

Every 1–3 months: check selectors, update scrapers when site changes.

Add new sources incrementally.




Estimated time: 1–4 hours monthly.


---

Timeline example (4–6 weeks, part-time)

Week 1: Milestones 0–2 (repo, DB, basic scraper test)
Week 2: Milestones 2–4 (finalize scraper, save DB, scheduler)
Week 3: Milestone 5 (frontend integration)
Week 4: Milestone 6 (testing + polish)
Week 5: Milestone 7 (deploy + CI)
Week 6: Buffer for bugs, selector fixes, and polish


---

Costs (concrete)

If you do it yourself (recommended):

Mentor / code review: €300–600 (1–2 months)

Infrastructure (monthly): €10–30

One-time domain: €10–20


If you hire a freelancer (outsourced per milestone approx):

Milestone 1–3 (backend core): €1,200 – 2,500

Milestone 4–7 (frontend + deploy): €1,200 – 3,000

Total outsourced: €2,500 – 6,000 (depending on rates and polish)



---

Security & legal checklist (must do)

1. Respect robots.txt and site terms. If a source forbids scraping, do not scrape it; use RSS or API alternatives.


2. Throttle requests, set a polite User-Agent.


3. Protect /api/news/scrape with SCRAPE_KEY or IP whitelist.


4. Avoid storing sensitive info in DB. Use env vars.




---

Quick copy-paste checklist to get started (first 48 hours)

1. Create backend folder and init: npm init -y


2. Install basics: npm i express mongoose dotenv axios cheerio node-cron


3. Add server.ts, .env.example, models/News.ts


4. Add simple services/newsScraper.ts with one source and GET /api/scrape/test


5. Run: npx ts-node-dev server.ts and open http://localhost:5000/api/scrape/test




---

If you want, I can now:

Produce the exact files for server.ts, models/News.ts, services/newsScraper.ts, and routes/news.ts (TypeScript) ready to paste into your project, or

Create a week-by-week checklist in a printable Markdown file for your personal tracker.


Which of those would you like me to generate right now?